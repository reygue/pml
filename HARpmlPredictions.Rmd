---
title: "PML Course Project"
author: "Reynold Guerrier"
date: "December 23, 2016"
output: html_document
---
## Abstract
The aim of this project is to use HAR Human Activity Recognition data set to make predictions. We will apply machine learning algorithm on 20 test cases available in the given data set to be able to submit our predictions. We decide to use random forest, boosting, linear discriminant and classification trees on 4 subsets of the training data. 

```{r, echo=FALSE }
rm(list = ls())
setwd("/DATA/PERSO/coursera/pml")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "/DATA/PERSO/coursera/pml/pml-training.csv", method ="libcurl" )

 download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "/DATA/PERSO/coursera/pml/pml-testing.csv", method = "libcurl")

```
## Cleaning and Explanatory Data
When checking the training and the testing data sets we find out there is a lot of "NA" and some "DIV/0" that must be stripped out because they will not impact the predictions. Other variables like names, identifiers and time aren't useful either  for our work. 


```{r, echo=FALSE}
rm(list = ls())
montrain   <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
montest <-read.csv("pml-testing.csv" , na.strings=c("NA", "#DIV/0!", ""))

```

```{r, echo=FALSE}
inTrain <- createDataPartition(montrain$classe, p=0.6, list=FALSE)
training <- montrain[inTrain, ]
testing <- montest[-inTrain, ]
dim(training); dim(testing)
``` 

Removing all near Zero variables

```{r, echo=FALSE}

nonzero = nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nonzero$nonzero==FALSE]

nonzero <- nearZeroVar(testing,saveMetrics=TRUE)
testing <- testing[,nonzero$nonzero==FALSE]
dim(training); dim(testing)
```
 with that done we end up with 2 cleaned data set of training and testing. The training contains a lot observations with more than 60 variables while the testing data set is 20. As variables from Column 1 to 7 are not relevant for our predictions, let's remove them
 
```{r, echo=FALSE}
 training1 <- training
for(i in 1:length(training)) {
    if( sum( is.na( training[, i] ) ) /nrow(training) >= .7) {
        for(j in 1:length(training1)) {
            if( length( grep(names(training[i]), names(training1)[j]) ) == 1)  {
                training1 <- training1[ , -j]
            }   
        } 
    }
}

training = training1
rm(training1)
```

## Model definition
Because of the size of the Training set, we prefer defining some subsets and apply the algorithms on each of them iteratively until we figure out which one allows us to get the best accuracy performance

```{r, echo=FALSE}
library(caret) 

library(randomForest)
library(ElemStatLearn) 
library(pgmm) 
library(rpart) 
library(gbm) 
library(e1071) 
library(splines) 
library(survival) 

```


 
 
 